# Hyper-Dimensional Hacking

Here's a little recap of what I've been up to lately in the world of adding/expanding holographic support to the Citra 3DS emulator, and other, similarly fun, mostly-emulation related things.

---

### Work in Progress

ðŸš§ Working on fixing "squished" (Half Side-by-Side) stereo mode by adding an additional "Full Side-by-Side" stereo mode to Citra. [yes, i necro'd an issue from 2018](https://github.com/citra-emu/citra/issues/4255#issuecomment-1279594312)

ðŸš§ continued work on our RGB + Depth shader which Holophone3d is going to use to test interpolating views views via point-clouds.

- [ ] make sure rgb and depth buffers maintain aspect ratios when scaled
- [ ] make sure rgb and depth buffers render correctly when bottom screen is enabled
- [ ] probably should cover the edge case of portrait-rotated games too, but i'm gonna punt on that for now 

### Next Steps
- [x] begin dumping shaders from games and see what they look like. find vertex shaders
- [ ] test modifying a simple vertex shader and see if the change can be seen
- [ ] try isolating the vertex change to a single eye
- [ ] try rendering a THIRD pass with the modified vertex shader
- [ ] make the process scalable to N-many additional views/offsets
- [ ] render a quilt with at least 4 unique views
- [ ] pass the quilt to looking glass display

> **!** this opens the door for potentially porting work to android version for leia displays / or just leia desktop displays on windows

> **!** this also opens the door for a VR viewing mode that could respond to 6-DOF headtracking

### Continued Exploration...

- [ ] look into how Dolphin achieves it's stereoscopic effects
- [ ] look into how Dolphin implements **Free Look**
- [ ] see how far interpolating from point-clouds can take us
- [ ] look into [DorsalVR](https://github.com/MichaelJW/DorsalVR)'s implementation

## Overview

Main Objective: **Generate Additional Views from Citra 3DS emulator for multi-scopic displays** 

My main focus continues to be attempting to generate additional 3D viewpoints from Citra Emu for use with multi-scopic displays like Leia Lume Pad, RED Hydrogen One, and Looking Glass Portrait, Looking Glass 4K, etc...

You might also call these holographic or light-field displays. 

<details>
<summary> **Why?** </summary>
I figured 3DS is a natural fit for this because, 1) it's how the original content was meant to be viewed, so the system has a concept of stereoscopy built-in, 2) it's relatively low power, which gives us extra headroom

Hopefully, as VR and holographic displays become more affordable and more main-stream, more people will get to revisit some of their favorite content in a new way.

I eventually want to see how this approach can apply to other emulators such as:
VirtualBoy, NES[^1], SNES, N64, PSX, PS2, you name it. But, we've gotta start somewhere.
</details>

## Approach

So, we're approaching this a few ways to see which sticks / looks the best. They all have advantages and trade-offs:

1. Depth Based reconstruction/interpolation
2. Geometry / Fragment Shader Based multi-pass rendering

The first step was untangling the depth-map data from Citra. #todo: write about that. it's a whole other topic.
> Before that I wrote a Basic Interlacing Shader. 
> During that I wrote an addon for making it so ReGlass & ReFract can ingest the normalized depth buffer
> See here for more: https://github.com/jakedowns/reshade-shaders/

Once we got _mostly_ reliable/usable separable Left & Right Depth Buffers, we then needed to start imagining how we could take advantage of the additional depth buffer that we have. 

![](https://i.imgur.com/dURJlI3.png)

One idea we have is to create point clouds from the left and right views, and then interpolate between them to create additional views. not sure how well that will work or not, but it's fun to explore. [Check out this video of it](https://twitter.com/jakedowns/status/1577232213954859008)

Most 2D -> 3D conversion post-processing pipelines use a single depth buffer, and then do a re-projection to achieve the parallax effect, which can lead to some strange artifacts.

There are a few offline conversion pipelines that use AI, but none I know of are fast enough yet for real-time applications like games

Searching around the ReShade Discord recently, I was impressed when I saw [Flugan's Geo3D](https://www.patreon.com/Flugan/posts) project, which takes a different approach. Right now it is built for DirectX, and Citra only as an OpenGL backend, for now[^2] 

After he was graciously willing to review his approach with me, it basically boils down to:

1. dump pre-compiled shaders
1. figure out which ones to modify (still not clear on discerning that. beyond trial and error testing, heuristic?)
1. apply offsets to end of vertex shaders
1. profit!
 
in his case he does alternate frame rendering, hopefully we don't have to do that, but, since citra can run >60fps, it wouldn't be too bad to collect 2 or 4 frames before displaying i guess :shrug:

Also, since we're working with an emulator, we have a little more insight into how the games are working with the gpu, and hopefully Dolphin's Free Look and Stereoscopic modes hint at us being able to do more beyond just working with the rgb+d channels.


## Research & Tangents

When I get a little burnt-out on fixing up the aspect-ratio math and the 8 permutations of letter-boxed and pillar-boxed layouts, I take a break looking forward to other hurdles we'll have to get over to get this pipeline working. There's a lot of great work out there, from passionate developers, who (a lot of the time) graciously share their work on Github, or are friendly enough to chat about their techniques and approaches in Discord (Shout-out to [Flugan](https://www.patreon.com/Flugan) and [Crosire](https://github.com/crosire/reshade) for not only making and sharing such amazing tools, but also taking the time to respond to my million and one questions) as well as the Citra devs for putting up with me haha ðŸ˜…

While researching more on this topic and past attempts at 2D -> 3D, 2D -> VR hooks using libraries like ReShade, I found this stack overflow question, ["intercept the opengl calls and make multi-viewpoints and multi-viewports"](https://stackoverflow.com/questions/20758268/intercept-the-opengl-calls-and-make-multi-viewpoints-and-multi-viewports) and learned about a program called [GL Intercept](https://github.com/dtrebilco/glintercept/releases). 


### GL Intercept

This is a wrapper that does what it says on the tin, intercepts GL calls for debugging. you can then have it Log out that data in all kinds of various ways to get an overall picture of what GL commands are running, with what data, or even narrow it down to the frame-by-frame level.

While I was hopeful this would "just work",  it didn't work with citra because it can only hook into the main thread, and citra's gl context is off the main thread (plus it doesn't support multiple GL contexts, which citra also uses). It might be worth comparing / contrasting the benefits of trying to get it working, vs. just sticking in the ReShade sandbox. GL Intercept seems to have some interesting features worth exploring, like a "free camera" mode. Check out the [Readme](https://github.com/dtrebilco/glintercept/blob/master/README.md)

Another technique mentioned in the SO answers, involves intercepting all the OpenGL calls, and then replaying them in a offscreen buffer. if we did that N-many times (perhaps in N-many threads) we might be able to render our multi-views that way.

### Dolphin Stereoscopy
<img src="https://i.imgur.com/y5q8x15.png" width="600" />

While digging into that, I remembered that Dolphin (GameCube,Wii emulator) has it's own stereoscopic mode built-in. Before I dove into seeing how it was implemented, i wanted to test it out. It seems to be geometry based, not depth-map based. I still have some spelunking to do to find out how they achieved that.

They also have a feature called [**Free Look**](https://wiki.dolphin-emu.org/index.php?title=Free_Look), which, can be mapped to Gyro data (for google cardboard for example) So, that is another implementation I can reference and dig more into.

I'm hoping whatever camera control tricks they're using on that hardware might be something similar to how 3DS's graphics are drawn. but we'll have to see.

Before I did much (read: "any") deep-diving into the Dolphin source code, I got sucked into a rabbit hole of playing Super Mario Sunshine with my Oculus Rift. 

The screen-space heat haze and water reflection effects definitely mess with it, but I found an Action Replay code to disable those, excited to see if that works. There's also apparently a widescreen and 60fps unlock code. OH and a SKIP CUTSCENES code! ðŸ˜®â€ðŸ’¨

So yeah, excited to pick up this thread soon...

## Side-Adventure: Switch Labo VR

While I was reviewing the "state of stereoscopy in emulators in 2022" I remembered that I had seen people posting VR recordings from the *Yuzu Switch emulator*, where they had jumped into the "Labo Toy-Con VR" modes in _Super Mario Odyssey_, and _Breath of the Wild_. So, I wanted to see how well they worked just for fun.

I haven't tested BotW yet, but I did get SMO's VR mode working. Honestly the "depthiness" wasn't great, and the level implmentation seems super weak (can't play the game in VR, just special small levels where you're in a fixed position)

The "head tracking" on a real switch is based on the JoyCon/Tablet gyro/accel data. So to simulate that, I had to use an android or ios device to pass _it's_ gyro/accelerometer data, which would either mean digging out my lost steam controller, or taping a phone to my headset :P 

Since I didn't have any WiiMotes or Joy-Cons around, I went down another rabbit hole to see if I could use the same UDP server technique [Cemu Motion Control Protocol](https://v1993.github.io/cemuhook-protocol/) the phones / steam controller-method used, and instead, pass the Oculus Rift Headset's rotation/position data to Yuzu. 

[As you can see in my twitter video, it ... basically _works_](https://twitter.com/jakedowns/status/1581210131211034625?s=20&t=mRmPYvzGbpXde-oEh_Xeiw) but... I still have some work to do WRT filtering the Quaternion OVR data into a format that gives consistent results on the Yuzu side. Currently, it's jittery, seems to flip out, and exhibits gimbal lock.

Anyway, it was a fun chance to use **Rust** and I found 2 crates i just needed to glue together to make it work, [`pad-motion`](https://github.com/zduny/pad-motion) and [`ovr-sys`](https://github.com/dylanede/ovr-sys)

It's kind of neat, it's just a tiny .exe file that connects to the Native C OVRLib, then every 10ms or so, fires off the current tracking info to a local UDP port for Yuzu to consume. Pretty simple. 

My work-in-progress code for that little side-project is here: [`rust-ovr-cemuhook`](https://github.com/jakedowns/rust-ovr-cemuhook)

![super mario in 3D!](https://user-images.githubusercontent.com/1683122/195977320-03bff96a-7705-46b5-9a3d-f5a1b1e1ab1e.png)

## ReShade Depth3D

While I haven't yet tested the _native_ vr mode in BotW (which is more of the full game, as i understand it, not just special VR-only levels), I _did_ revisit the [Depth3D / SuperDepth_VR](https://github.com/BlueSkyDefender/Depth3D) shaders, using Breath of the Wild in Cemu (Wii U emulator) and was pretty impressed by the results. while i don't know how they might scale to multiscopy (>2 views), it's pretty convincing in stereo with the vr headset on. Tho, once again, I found myself wanting good head-tracking. I kept looking around expecting the game camera to move. Lol. Maybe Free Look + HMD tracking can be reused here? We'll have to see...

### Footnotes

[^1]: [3DSEN](http://www.geodstudio.net/) is an interesting project that has special per-game per-sprite meta-data for rendering NES games in 3D, passing the sprite info to a Unity runtime which then renders a custom 3D version of the game. in real-time. 
[^2]: **i** Note: Emufan _is_ currently writing a Vulkan backend for Citra, which might be easier to port Geo3D's geometry-based / vertex-shader-shifting technique to, rather than opengl, but since it's still in early testing, i'm going to continue focusing on implementing multiscopy with the opengl backend before going on another tangent.
